{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4497df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOAD DATA PEMBELIAN DAN STOK\n",
      "======================================================================\n",
      "✓ Data mentah pembelian: 138364 transaksi\n",
      "✓ Data stok dimuat: 1518 produk\n",
      "\n",
      "======================================================================\n",
      "AGREGASI MINIMAL PER PRODUK (untuk merge dengan stok)\n",
      "======================================================================\n",
      "✓ Mengambil transaksi TERAKHIR per produk: 2024 produk\n",
      "✓ Data merged: 359 produk\n",
      "\n",
      "======================================================================\n",
      "FASE 1: DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "[1.1] Data Formatting\n",
      "✓ Data Formatting selesai\n",
      "✓ Jumlah fitur: 10\n",
      "✓ Jumlah sampel: 359\n",
      "✓ Distribusi target: {0: 350, 1: 6, 2: 3}\n",
      "\n",
      "[1.2] Data Scaling (Standardization)\n",
      "✓ Standardization: mean=0, std=1\n",
      "\n",
      "[1.3] Data Randomization\n",
      "✓ Data shuffled untuk menghindari bias\n",
      "\n",
      "======================================================================\n",
      "FASE 2: FEATURE REDUCTION USING BFS-RST\n",
      "======================================================================\n",
      "\n",
      "  Executing BFS-RST...\n",
      "  → Computing Core Attributes (RST)...\n",
      "  → Core attributes: 3 features\n",
      "  → BFS iterations (max: 30)...\n",
      "  ✓ BFS-RST completed: 1 iterations\n",
      "  ✓ Features selected: 3\n",
      "\n",
      "✓ Feature Reduction Done!\n",
      "  Original: 10 features\n",
      "  Reduced: 3 features\n",
      "  Selected: ['Qty_Masuk', 'Nilai_Masuk', 'Stok_Aktual']\n",
      "\n",
      "======================================================================\n",
      "FASE 3: FEATURE SELECTION USING DCRRF\n",
      "======================================================================\n",
      "\n",
      "  Executing DCRRF...\n",
      "  → Training 50 trees...\n",
      "    → 10/50 trees trained\n",
      "    → 20/50 trees trained\n",
      "    → 30/50 trees trained\n",
      "    → 40/50 trees trained\n",
      "    → 50/50 trees trained\n",
      "\n",
      "  ✓ DCRRF completed!\n",
      "  ✓ Optimal features: 2\n",
      "\n",
      "✓ Feature Selection Done!\n",
      "  After BFS-RST: 3 features\n",
      "  After DCRRF: 2 features\n",
      "  Final features: ['Qty_Masuk', 'Stok_Aktual']\n",
      "\n",
      "======================================================================\n",
      "FASE 4: DATA ANALYSIS USING SVM\n",
      "======================================================================\n",
      "\n",
      "[4.1] Train-Test Split\n",
      "  Train: 287 samples\n",
      "  Test: 72 samples\n",
      "\n",
      "[4.2] SVM Hyperparameters (Table II):\n",
      "  - Kernel: RBF\n",
      "  - C: 1\n",
      "  - Max iterations: 100\n",
      "\n",
      "======================================================================\n",
      "HASIL AKHIR - PERFORMANCE COMPARISON (Table III Format)\n",
      "======================================================================\n",
      "\n",
      "Model                FS    Accuracy   Sensitivity  Specificity  Precision  F1-score  \n",
      "==========================================================================================\n",
      "BFSRST+DCRRF         2     0.9861     0.9861       0.8333       0.9724    0.9792\n",
      "\n",
      "✓ Feature Selection Summary:\n",
      "  - Original features: 10\n",
      "  - After BFS-RST: 3\n",
      "  - After DCRRF: 2\n",
      "  - Reduction: 8 features (80.0%)\n",
      "\n",
      "✓ Final Selected Features:\n",
      "  1. Qty_Masuk                 (selected in 80.0% of trees)\n",
      "  2. Stok_Aktual               (selected in 90.0% of trees)\n",
      "\n",
      "======================================================================\n",
      "COMPARISON WITH BASELINE (All Features)\n",
      "======================================================================\n",
      "\n",
      "Model                Features   Accuracy     Precision    Sensitivity  F1-Score  \n",
      "================================================================================\n",
      "Baseline (All)       10         0.9722       0.9452       0.9722       0.9585\n",
      "BFSRST+DCRRF         2          0.9861       0.9724       0.9861       0.9792\n",
      "\n",
      "✓ Improvement:\n",
      "  - Feature reduction: 80.0%\n",
      "  - Accuracy change: +0.0139 (+1.43%)\n",
      "  - F1-Score change: +0.0207 (+2.16%)\n",
      "\n",
      "======================================================================\n",
      "✓ IMPLEMENTASI JURNAL SELESAI!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================\n",
    "# LOAD DATA (Preprocessing Awal)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOAD DATA PEMBELIAN DAN STOK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. PARSING DATA PEMBELIAN\n",
    "data = []\n",
    "kode, nama, unit = None, None, None\n",
    "with open('dataset-apotek-pembelian.tsv', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or set(line) == {'-'}:\n",
    "            continue\n",
    "        if re.match(r'^[A-Z0-9]{5,}\\s+', line):\n",
    "            parts = re.split(r'\\s{2,}', line)\n",
    "            kode = parts[0].strip()\n",
    "            nama = parts[1].strip() if len(parts) > 1 else None\n",
    "            unit = parts[-1].strip() if len(parts) > 2 else None\n",
    "            continue\n",
    "        if re.match(r'^\\d{2}-\\d{2}-\\d{2}', line):\n",
    "            tanggal = line[0:8].strip()\n",
    "            no_transaksi = line[9:35].strip()\n",
    "            qty_masuk = line[36:47].strip()\n",
    "            nilai_masuk = line[48:61].strip()\n",
    "            qty_keluar = line[62:73].strip()\n",
    "            nilai_keluar = line[74:].strip()\n",
    "            data.append([kode, nama, unit, tanggal, no_transaksi, qty_masuk, nilai_masuk, qty_keluar, nilai_keluar])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'Kode', 'Nama_Produk', 'Unit', 'Tanggal', 'No_Transaksi',\n",
    "    'Qty_Masuk', 'Nilai_Masuk', 'Qty_Keluar', 'Nilai_Keluar'\n",
    "])\n",
    "\n",
    "def to_float(val):\n",
    "    val = str(val).replace('.', '').replace(',', '.')\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "for c in ['Qty_Masuk', 'Nilai_Masuk', 'Qty_Keluar', 'Nilai_Keluar']:\n",
    "    df[c] = df[c].apply(to_float)\n",
    "\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'], format='%d-%m-%y', errors='coerce')\n",
    "df = df.dropna(subset=['Tanggal'])\n",
    "\n",
    "# Tambahkan fitur temporal SEDERHANA (bukan agregasi)\n",
    "df['Bulan'] = df['Tanggal'].dt.month\n",
    "df['Tahun'] = df['Tanggal'].dt.year\n",
    "df['Hari'] = df['Tanggal'].dt.day\n",
    "df['Hari_dalam_Minggu'] = df['Tanggal'].dt.dayofweek\n",
    "\n",
    "print(f\"✓ Data mentah pembelian: {len(df)} transaksi\")\n",
    "\n",
    "# 2. PARSING DATA STOK\n",
    "df_stok = pd.read_fwf('dataset-apotek-stok.tsv', encoding='utf-8')\n",
    "df_stok = df_stok.dropna(axis=1, how='all')\n",
    "df_stok = df_stok.loc[:, ~df_stok.columns.str.contains('Unnamed', case=False)]\n",
    "df_stok.columns = (\n",
    "    df_stok.columns.str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace(' ', '_', regex=False)\n",
    ")\n",
    "\n",
    "stok_col = [col for col in df_stok.columns if 'QTY' in col and 'STOK' in col]\n",
    "if not stok_col:\n",
    "    raise KeyError(f\"Kolom stok tidak ditemukan!\")\n",
    "stok_col = stok_col[0]\n",
    "\n",
    "df_stok = df_stok[~df_stok[stok_col].astype(str).str.contains('-', regex=False, na=False)]\n",
    "df_stok = df_stok[df_stok[stok_col].astype(str).str.strip() != '']\n",
    "df_stok[stok_col] = (\n",
    "    df_stok[stok_col]\n",
    "    .astype(str)\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace(',', '.', regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_stok = df_stok.rename(columns={\n",
    "    'KODE': 'Kode',\n",
    "    'NAMA_PRODUK': 'Nama_Produk',\n",
    "    'LOKASI': 'Lokasi',\n",
    "    stok_col: 'Stok_Aktual',\n",
    "    'UNIT': 'Unit'\n",
    "})\n",
    "\n",
    "print(f\"✓ Data stok dimuat: {len(df_stok)} produk\")\n",
    "\n",
    "# 3. AGREGASI MINIMAL (HANYA UNTUK MERGE)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGREGASI MINIMAL PER PRODUK (untuk merge dengan stok)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pembelian_simple = df.sort_values(['Kode', 'Tanggal']).groupby('Kode').tail(1).reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Mengambil transaksi TERAKHIR per produk: {len(pembelian_simple)} produk\")\n",
    "\n",
    "# Merge dengan stok\n",
    "df_merged = pembelian_simple.merge(df_stok[['Kode', 'Stok_Aktual', 'Lokasi']], on='Kode', how='inner')\n",
    "\n",
    "print(f\"✓ Data merged: {len(df_merged)} produk\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# FASE 1: DATA PREPROCESSING (Jurnal Section IV.A)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FASE 1: DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1.1 Data Formatting\n",
    "print(\"\\n[1.1] Data Formatting\")\n",
    "\n",
    "# Encode Lokasi\n",
    "le_lokasi = LabelEncoder()\n",
    "df_merged['Lokasi_Encoded'] = le_lokasi.fit_transform(df_merged['Lokasi'].astype(str))\n",
    "\n",
    "# Buat target: Kategori stok (Fast/Medium/Slow moving)\n",
    "df_merged['Target'] = pd.cut(\n",
    "    df_merged['Stok_Aktual'], \n",
    "    bins=3, \n",
    "    labels=[0, 1, 2]  # 0=Low, 1=Medium, 2=High stock\n",
    ")\n",
    "\n",
    "# Pilih fitur LANGSUNG dari data (TIDAK ada agregasi)\n",
    "feature_cols = [\n",
    "    'Qty_Masuk', 'Nilai_Masuk', 'Qty_Keluar', 'Nilai_Keluar',\n",
    "    'Bulan', 'Tahun', 'Hari', 'Hari_dalam_Minggu', \n",
    "    'Stok_Aktual', 'Lokasi_Encoded'\n",
    "]\n",
    "\n",
    "X = df_merged[feature_cols].copy()\n",
    "y = df_merged['Target'].copy()\n",
    "\n",
    "# Remove missing\n",
    "valid_idx = ~y.isna()\n",
    "X = X[valid_idx].reset_index(drop=True)\n",
    "y = y[valid_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Data Formatting selesai\")\n",
    "print(f\"✓ Jumlah fitur: {X.shape[1]}\")\n",
    "print(f\"✓ Jumlah sampel: {X.shape[0]}\")\n",
    "print(f\"✓ Distribusi target: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# 1.2 Data Scaling (Standardization - Equation 22 jurnal)\n",
    "print(\"\\n[1.2] Data Scaling (Standardization)\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n",
    "\n",
    "print(f\"✓ Standardization: mean=0, std=1\")\n",
    "\n",
    "# 1.3 Data Randomization\n",
    "print(\"\\n[1.3] Data Randomization\")\n",
    "\n",
    "y_encoded = y.astype(int).values\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(len(X_scaled))\n",
    "X_scaled = X_scaled.iloc[shuffle_idx].reset_index(drop=True)\n",
    "y_encoded = y_encoded[shuffle_idx]\n",
    "\n",
    "print(f\"✓ Data shuffled untuk menghindari bias\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# FASE 2: FEATURE REDUCTION (BFS-RST - Algorithm 3)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FASE 2: FEATURE REDUCTION USING BFS-RST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class BFS_RST_FeatureReduction:\n",
    "    \"\"\"\n",
    "    Best-First Search + Rough Set Theory\n",
    "    Implementasi Algorithm 3 dari jurnal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_features=3):\n",
    "        self.min_features = min_features\n",
    "        self.selected_features = None\n",
    "        self.core_features = None\n",
    "    \n",
    "    def _compute_core_attributes(self, X, y):\n",
    "        \"\"\"\n",
    "        Step: Compute core attributes using RST (Equation 17)\n",
    "        Core = {a ∈ C : RED(C) ≠ RED(C - {a})}\n",
    "        Approximation: gunakan mutual information\n",
    "        \"\"\"\n",
    "        print(\"  → Computing Core Attributes (RST)...\")\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        threshold = np.percentile(mi_scores, 70)\n",
    "        core = np.where(mi_scores >= threshold)[0]\n",
    "        print(f\"  → Core attributes: {len(core)} features\")\n",
    "        return core, mi_scores\n",
    "    \n",
    "    def _evaluation_function(self, X_subset, y, features):\n",
    "        \"\"\"\n",
    "        Evaluation function f(N) = g(N) + h(N) (Equation 20)\n",
    "        \"\"\"\n",
    "        if len(features) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\n",
    "        rf.fit(X_subset, y)\n",
    "        acc = rf.score(X_subset, y)\n",
    "        \n",
    "        # f(N) = alpha*cost + beta*error\n",
    "        cost = len(features) / X_subset.shape[1]\n",
    "        error = 1 - acc\n",
    "        return 0.3 * cost + 0.7 * error\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Main BFS-RST Algorithm\n",
    "        \"\"\"\n",
    "        print(\"\\n  Executing BFS-RST...\")\n",
    "        \n",
    "        # Initialize: Core attributes\n",
    "        core, mi_scores = self._compute_core_attributes(X, y)\n",
    "        self.core_features = core\n",
    "        \n",
    "        # Priority Queue\n",
    "        import heapq\n",
    "        pq = []\n",
    "        \n",
    "        X_core = X.iloc[:, core]\n",
    "        eval_core = self._evaluation_function(X_core, y, core)\n",
    "        heapq.heappush(pq, (eval_core, tuple(core)))\n",
    "        \n",
    "        visited = set()\n",
    "        best_features = core\n",
    "        best_score = eval_core\n",
    "        \n",
    "        max_iter = 30\n",
    "        iteration = 0\n",
    "        \n",
    "        print(f\"  → BFS iterations (max: {max_iter})...\")\n",
    "        \n",
    "        # BFS Loop\n",
    "        while pq and iteration < max_iter:\n",
    "            curr_score, curr_feat = heapq.heappop(pq)\n",
    "            curr_feat = list(curr_feat)\n",
    "            \n",
    "            feat_tuple = tuple(sorted(curr_feat))\n",
    "            if feat_tuple in visited:\n",
    "                continue\n",
    "            visited.add(feat_tuple)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            # Stopping criteria\n",
    "            if len(curr_feat) <= self.min_features:\n",
    "                if curr_score < best_score:\n",
    "                    best_score = curr_score\n",
    "                    best_features = curr_feat\n",
    "                break\n",
    "            \n",
    "            # Generate child nodes (FR: remove one feature)\n",
    "            for f in curr_feat:\n",
    "                child = [x for x in curr_feat if x != f]\n",
    "                \n",
    "                if len(child) < self.min_features:\n",
    "                    continue\n",
    "                \n",
    "                child_tuple = tuple(sorted(child))\n",
    "                if child_tuple in visited:\n",
    "                    continue\n",
    "                \n",
    "                X_child = X.iloc[:, child]\n",
    "                child_score = self._evaluation_function(X_child, y, child)\n",
    "                \n",
    "                # Priority adjustment if reduct\n",
    "                avg_mi_child = np.mean(mi_scores[child])\n",
    "                avg_mi_all = np.mean(mi_scores)\n",
    "                if avg_mi_child >= 0.8 * avg_mi_all:\n",
    "                    child_score *= 0.8  # High priority\n",
    "                \n",
    "                heapq.heappush(pq, (child_score, tuple(child)))\n",
    "                \n",
    "                if child_score < best_score:\n",
    "                    best_score = child_score\n",
    "                    best_features = child\n",
    "        \n",
    "        self.selected_features = best_features\n",
    "        print(f\"  ✓ BFS-RST completed: {iteration} iterations\")\n",
    "        print(f\"  ✓ Features selected: {len(self.selected_features)}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Execute BFS-RST\n",
    "bfs_rst = BFS_RST_FeatureReduction(min_features=3)\n",
    "bfs_rst.fit(X_scaled, y_encoded)\n",
    "\n",
    "X_reduced = X_scaled.iloc[:, bfs_rst.selected_features]\n",
    "reduced_names = [feature_cols[i] for i in bfs_rst.selected_features]\n",
    "\n",
    "print(f\"\\n✓ Feature Reduction Done!\")\n",
    "print(f\"  Original: {X_scaled.shape[1]} features\")\n",
    "print(f\"  Reduced: {X_reduced.shape[1]} features\")\n",
    "print(f\"  Selected: {reduced_names}\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# FASE 3: FEATURE SELECTION (DCRRF - Algorithm 4)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FASE 3: FEATURE SELECTION USING DCRRF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class DCRRF:\n",
    "    \"\"\"\n",
    "    Dynamic Correlated Regularized Random Forest\n",
    "    Implementasi Algorithm 4 dari jurnal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=50, lambda_reg=0.01, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.random_state = random_state\n",
    "        self.feature_sets = []\n",
    "        self.optimal_features = None\n",
    "        self.feature_freq = None\n",
    "    \n",
    "    def _cfs_merit(self, X, y, features):\n",
    "        \"\"\"\n",
    "        CFS criterion (Equation 18, 25)\n",
    "        Merit_S = k*rcf / sqrt(k + k(k-1)*rff)\n",
    "        \"\"\"\n",
    "        if len(features) == 0:\n",
    "            return 0\n",
    "        \n",
    "        X_sub = X.iloc[:, features]\n",
    "        k = len(features)\n",
    "        \n",
    "        # rcf: correlation with class\n",
    "        mi = mutual_info_classif(X_sub, y, random_state=self.random_state)\n",
    "        rcf = np.mean(mi)\n",
    "        \n",
    "        # rff: inter-feature correlation\n",
    "        if k > 1:\n",
    "            corr = X_sub.corr().abs()\n",
    "            mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "            rff = corr.where(mask).stack().mean()\n",
    "        else:\n",
    "            rff = 0\n",
    "        \n",
    "        denom = np.sqrt(k + k * (k - 1) * rff)\n",
    "        merit = (k * rcf) / denom if denom > 0 else 0\n",
    "        \n",
    "        return merit\n",
    "    \n",
    "    def _select_features_cfs(self, X, y, max_features):\n",
    "        \"\"\"\n",
    "        Greedy forward selection using CFS\n",
    "        \"\"\"\n",
    "        selected = []\n",
    "        remaining = list(range(X.shape[1]))\n",
    "        \n",
    "        for _ in range(min(max_features, len(remaining))):\n",
    "            best_merit = -1\n",
    "            best_feat = None\n",
    "            \n",
    "            for f in remaining[:5]:  # Limit search untuk efisiensi\n",
    "                candidate = selected + [f]\n",
    "                merit = self._cfs_merit(X, y, candidate)\n",
    "                \n",
    "                if merit > best_merit:\n",
    "                    best_merit = merit\n",
    "                    best_feat = f\n",
    "            \n",
    "            if best_feat is not None:\n",
    "                selected.append(best_feat)\n",
    "                remaining.remove(best_feat)\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Main DCRRF Algorithm\n",
    "        \"\"\"\n",
    "        print(\"\\n  Executing DCRRF...\")\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        self.feature_freq = np.zeros(n_features)\n",
    "        \n",
    "        print(f\"  → Training {self.n_estimators} trees...\")\n",
    "        \n",
    "        for t in range(self.n_estimators):\n",
    "            # Bootstrap Sample\n",
    "            boot_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_boot = X.iloc[boot_idx]\n",
    "            y_boot = y[boot_idx]\n",
    "            \n",
    "            # Dynamic FS dengan CFS\n",
    "            selected = self._select_features_cfs(\n",
    "                X_boot, y_boot, \n",
    "                max_features=max(2, n_features // 2)\n",
    "            )\n",
    "            \n",
    "            self.feature_sets.append(selected)\n",
    "            \n",
    "            for f in selected:\n",
    "                self.feature_freq[f] += 1\n",
    "            \n",
    "            if (t + 1) % 10 == 0:\n",
    "                print(f\"    → {t+1}/{self.n_estimators} trees trained\")\n",
    "        \n",
    "        # Determine Optimal Features (Equation 27: Intersection)\n",
    "        # Pilih fitur yang muncul di >= 50% trees\n",
    "        threshold = self.n_estimators * 0.5\n",
    "        self.optimal_features = np.where(self.feature_freq >= threshold)[0]\n",
    "        \n",
    "        if len(self.optimal_features) < 2:\n",
    "            # Fallback: ambil top features\n",
    "            self.optimal_features = np.argsort(self.feature_freq)[-3:]\n",
    "        \n",
    "        print(f\"\\n  ✓ DCRRF completed!\")\n",
    "        print(f\"  ✓ Optimal features: {len(self.optimal_features)}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Execute DCRRF\n",
    "dcrrf = DCRRF(n_estimators=50, lambda_reg=0.01, random_state=42)\n",
    "dcrrf.fit(X_reduced, y_encoded)\n",
    "\n",
    "X_final = X_reduced.iloc[:, dcrrf.optimal_features]\n",
    "final_names = [reduced_names[i] for i in dcrrf.optimal_features]\n",
    "\n",
    "print(f\"\\n✓ Feature Selection Done!\")\n",
    "print(f\"  After BFS-RST: {X_reduced.shape[1]} features\")\n",
    "print(f\"  After DCRRF: {X_final.shape[1]} features\")\n",
    "print(f\"  Final features: {final_names}\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# FASE 4: DATA ANALYSIS (SVM - Section IV.D)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FASE 4: DATA ANALYSIS USING SVM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split 80:20 (Table II jurnal)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n[4.1] Train-Test Split\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "\n",
    "# SVM dengan hyperparameter Table II\n",
    "print(f\"\\n[4.2] SVM Hyperparameters (Table II):\")\n",
    "print(f\"  - Kernel: RBF\")\n",
    "print(f\"  - C: 1\")\n",
    "print(f\"  - Max iterations: 100\")\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=1, max_iter=100, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Metrics (Table III)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "sensitivity = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "# Specificity (manual)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "specificity_list = []\n",
    "for i in range(len(cm)):\n",
    "    tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "    fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    specificity_list.append(spec)\n",
    "specificity = np.mean(specificity_list)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# HASIL AKHIR (Format Table III Jurnal)\n",
    "# ==============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HASIL AKHIR - PERFORMANCE COMPARISON (Table III Format)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'FS':<5} {'Accuracy':<10} {'Sensitivity':<12} {'Specificity':<12} {'Precision':<10} {'F1-score':<10}\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'BFSRST+DCRRF':<20} {len(final_names):<5} {accuracy:.4f}     {sensitivity:.4f}       {specificity:.4f}       {precision:.4f}    {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Feature Selection Summary:\")\n",
    "print(f\"  - Original features: {len(feature_cols)}\")\n",
    "print(f\"  - After BFS-RST: {len(reduced_names)}\")\n",
    "print(f\"  - After DCRRF: {len(final_names)}\")\n",
    "print(f\"  - Reduction: {len(feature_cols) - len(final_names)} features ({((len(feature_cols)-len(final_names))/len(feature_cols)*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\n✓ Final Selected Features:\")\n",
    "for i, feat in enumerate(final_names, 1):\n",
    "    freq = dcrrf.feature_freq[dcrrf.optimal_features[i-1]]\n",
    "    pct = (freq / dcrrf.n_estimators) * 100\n",
    "    print(f\"  {i}. {feat:<25} (selected in {pct:.1f}% of trees)\")\n",
    "\n",
    "# Baseline comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON WITH BASELINE (All Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "svm_baseline = SVC(kernel='rbf', C=1, max_iter=100, random_state=42)\n",
    "svm_baseline.fit(X_train_all, y_train_all)\n",
    "y_pred_base = svm_baseline.predict(X_test_all)\n",
    "\n",
    "acc_base = accuracy_score(y_test_all, y_pred_base)\n",
    "prec_base = precision_score(y_test_all, y_pred_base, average='weighted', zero_division=0)\n",
    "sens_base = recall_score(y_test_all, y_pred_base, average='weighted', zero_division=0)\n",
    "f1_base = f1_score(y_test_all, y_pred_base, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'Features':<10} {'Accuracy':<12} {'Precision':<12} {'Sensitivity':<12} {'F1-Score':<10}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Baseline (All)':<20} {len(feature_cols):<10} {acc_base:.4f}       {prec_base:.4f}       {sens_base:.4f}       {f1_base:.4f}\")\n",
    "print(f\"{'BFSRST+DCRRF':<20} {len(final_names):<10} {accuracy:.4f}       {precision:.4f}       {sensitivity:.4f}       {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Improvement:\")\n",
    "print(f\"  - Feature reduction: {((len(feature_cols)-len(final_names))/len(feature_cols)*100):.1f}%\")\n",
    "print(f\"  - Accuracy change: {(accuracy-acc_base):+.4f} ({((accuracy-acc_base)/acc_base*100):+.2f}%)\")\n",
    "print(f\"  - F1-Score change: {(f1-f1_base):+.4f} ({((f1-f1_base)/f1_base*100):+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ IMPLEMENTASI JURNAL SELESAI!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
